# Bank-Marketing-ML-Project
Data Preprocessing &amp; Feature Engineering Report

# ðŸ“Š Bank Marketing ML Project â€“ Data Preprocessing & Feature Engineering Report

## ðŸ“Œ Project Overview

This project focuses on preparing the **Bank Marketing dataset** for machine learning by applying correct **data cleaning, feature selection, encoding, and transformation techniques**. The goal is to build a **leakage-free, production-ready preprocessing pipeline** suitable for real-world ML models.

---

## ðŸŽ¯ Objective

* Remove leaky and redundant features
* Handle multicollinearity
* Encode categorical variables correctly
* Prevent data leakage
* Prepare model-ready numerical data

---

## ðŸ—‚ Dataset Description

The dataset contains customer information, campaign details, and macroeconomic indicators collected during a bankâ€™s direct marketing campaigns.

**Target Variable:**

* `y` â†’ Whether the client subscribed to a term deposit (`yes` / `no`)

---

## ðŸ§¹ Feature Selection & Column Dropping

### âŒ Dropped Feature

* **`duration`**

  * Reason: **Data Leakage**
  * The call duration is only known *after* the call ends and cannot be used for real-time prediction.

### âŒ Dropped Highly Correlated Features

Based on correlation analysis:

* `nr.employed`
* `cons.price.idx`

These variables were highly correlated with other macroeconomic indicators and added redundant information.

---

## ðŸ“Š Final Feature Set

### ðŸ”¢ Numerical Features

* `age`
* `campaign`
* `pdays`
* `previous`
* `cons.conf.idx`
* `euribor3m`

### ðŸ”  Categorical Features

* `job`
* `marital`
* `education`
* `housing`
* `loan`
* `contact`
* `month`
* `poutcome`

---

## ðŸ§  Feature Engineering Strategy

> **Note:** A full `Pipeline` was not used in this project. Instead, a `ColumnTransformer` was applied directly after the trainâ€“test split to keep preprocessing explicit and easy to understand.

### 1ï¸âƒ£ Ordinal Encoding

Applied to the `education` column due to its inherent order:

```
university.degree > professional.course > high.school > basic.9y > basic.6y > basic.4y > illiterate > unknown
```

Used **OrdinalEncoder** with explicitly defined category order.

---

### 2ï¸âƒ£ One-Hot Encoding

Applied to nominal categorical variables using **OneHotEncoder** with:

* `handle_unknown='ignore'`
* `drop='first'` to avoid dummy variable trap

---

### 3ï¸âƒ£ ColumnTransformer

A **ColumnTransformer** was used directly (without wrapping it inside a Pipeline) to apply different transformations to different feature groups.

This approach was chosen to:

* Keep preprocessing steps transparent
* Avoid abstraction while learning core ML concepts
* Explicitly control `fit` and `transform` on train and test data

The transformer was **fitted only on training data** and applied to test data separately, ensuring no data leakage.

---

## ðŸ”€ Train-Test Split

The dataset was split **before fitting any transformer** using the following configuration:

* **70% Training data**
* **30% Testing data** (`test_size = 0.3`)
* Stratified on the target variable to preserve class distribution

This ensured that preprocessing decisions were learned only from training data and prevented information leakage from the test set.

---

## ðŸŽ¯ Target Encoding

The target variable `y` was encoded using **LabelEncoder**:

```
no  â†’ 0
yes â†’ 1
```

Encoding was fitted **only on training data** and applied to the test set.

---

## âœ… Final Preprocessing Checklist

âœ” Leaky feature removed (`duration`)
âœ” Multicollinearity handled
âœ” Correct encoding strategies applied
âœ” Train-test leakage prevented
âœ” Model-ready numerical dataset created

---

## ðŸš€ Next Steps

* Train baseline models (Logistic Regression)
* Try tree-based models (Random Forest, Gradient Boosting)
* Handle class imbalance
* Evaluate using ROC-AUC, Precision, Recall, and Confusion Matrix

---

## ðŸ§  Key Learning

> *Feature engineering is not about writing more code, but about making correct data decisions.*

---

ðŸ“Œ **This preprocessing pipeline is suitable for both academic submission and real-world ML deployment.**


